# Pipeline de Dados Local com DBT (POC sem Airbyte)

Este projeto implementa um pipeline de dados simplificado para demonstraÃ§Ã£o local, conectando DBT diretamente ao banco de origem para transformaÃ§Ãµes em camadas (Bronze, Silver, Gold), sem a complexidade do Airbyte.

## ğŸ—ï¸ Arquitetura Simplificada

```
[Script InserÃ§Ã£o] â†’ [PostgreSQL Source] â†’ [DBT] â†’ [Camadas Bronze/Silver/Gold]
                                            â†“
                                      [MinIO Data Lake]
```

### Componentes:

- **PostgreSQL Source**: Banco transacional com dados simulados
- **Script de InserÃ§Ã£o**: Simula dados sendo inseridos continuamente
- **DBT**: Ferramenta de transformaÃ§Ã£o conectada diretamente ao banco de origem
- **MinIO**: Data Lake para armazenamento de arquivos
- **Scheduler DBT**: Script Python para execuÃ§Ã£o automÃ¡tica do pipeline

### Vantagens desta Abordagem para POC:

- âœ… **Simplicidade**: Menos componentes para gerenciar
- âœ… **Foco no DBT**: Demonstra claramente as capacidades de transformaÃ§Ã£o
- âœ… **Setup RÃ¡pido**: InicializaÃ§Ã£o em minutos
- âœ… **Recursos MÃ­nimos**: Menor consumo de CPU/memÃ³ria
- âœ… **Ideal para DemonstraÃ§Ãµes**: FÃ¡cil de explicar e entender

## ğŸ” **Credenciais Padronizadas**

**Todos os serviÃ§os usam:**
```
UsuÃ¡rio: admin
Senha: admin
```

## ğŸš€ **InicializaÃ§Ã£o RÃ¡pida**

```bash
# 1. Clone e acesse o diretÃ³rio
git clone <repo-url>
cd dbt

# 2. Inicie o pipeline simplificado
./scripts/start_dbt_pipeline.sh

# 3. Aguarde a inicializaÃ§Ã£o (1-2 minutos)
# O script irÃ¡:
# - Subir PostgreSQL Source
# - Configurar DBT para conectar diretamente
# - Executar DBT (Bronze â†’ Silver â†’ Gold)
# - Iniciar inserÃ§Ã£o contÃ­nua de dados

# 4. Para execuÃ§Ã£o automÃ¡tica contÃ­nua
python scripts/scheduler_dbt.py

# 5. Acesse as interfaces
# MinIO: http://localhost:9001 (admin/admin123)
# PostgreSQL: localhost:5430 (admin/admin)
```

## ğŸ¯ **InÃ­cio RÃ¡pido**

### 1ï¸âƒ£ **Executar Pipeline Completo**
```bash
./start_pipeline.sh
```

### 2ï¸âƒ£ **Executar Pipeline DBT**

> âœ… **Simples**: O pipeline agora conecta diretamente ao banco de origem, sem necessidade de configuraÃ§Ã£o adicional.

1. **Execute o script de inicializaÃ§Ã£o**:
   ```bash
   ./scripts/start_dbt_pipeline.sh
   ```

2. **Ou execute manualmente**:
   ```bash
   # Subir infraestrutura
   docker compose up -d
   
   # Inserir dados de exemplo
   python3 scripts/insere_dados.py
   
   # Executar DBT
   docker compose exec dbt_runner dbt run
   ```

### 3ï¸âƒ£ **Executar DBT (ApÃ³s CDC configurado)**
```bash
python3 scripts/executar_dbt.py debug    # Testar conexÃ£o
python3 scripts/executar_dbt.py bronze   # Modelos bronze
python3 scripts/executar_dbt.py silver   # Modelos silver
python3 scripts/executar_dbt.py gold     # Modelos gold
python3 scripts/executar_dbt.py full     # Pipeline completo
```

### 4ï¸âƒ£ **Limpeza Completa (quando quiser resetar tudo)**
```bash
./clean_docker_environment.sh
```
**âš ï¸ CUIDADO:** Remove TUDO - containers, volumes, dados, configuraÃ§Ãµes!

## ï¿½ï¿½ï¸ **Arquitetura**

### **Abordagem HÃ­brida: Docker + Python**
- **Docker**: Apenas para infraestrutura (PostgreSQL, MinIO, DBT Runner)
- **Python**: ExecuÃ§Ã£o de lÃ³gica (DBT, verificaÃ§Ãµes, criaÃ§Ã£o de tabelas)
- **Credenciais Padronizadas**: admin/admin para todos os serviÃ§os

### **Fluxo de Dados**
```
1. PostgreSQL Source (dados originais)
   â†“
2. DBT para transformaÃ§Ãµes de dados
   â†“
3. PostgreSQL Target (dados replicados)
   â†“
4. DBT Python (transformaÃ§Ãµes)
   â†“
5. Dashboard Streamlit
```

## ğŸ“Š **Estrutura de Dados**

### **Tabelas Principais:**
- **clientes** - Dados de clientes com perfil empresarial
- **pedidos** - Pedidos sem itens (estrutura empresarial)
- **produtos** - CatÃ¡logo de produtos e-commerce
- **itens_pedido** - Relacionamento produtosâ†”pedidos
- **campanhas_marketing** - Campanhas de marketing
- **leads** - Leads gerados pelas campanhas

### **Camadas DBT:**
- **ğŸ¥‰ Bronze** - Dados brutos do banco de origem
- **ğŸ¥ˆ Silver** - Dados limpos e padronizados
- **ğŸ¥‡ Gold** - AgregaÃ§Ãµes e mÃ©tricas de negÃ³cio

## ğŸŒ **URLs dos ServiÃ§os**

| ServiÃ§o | URL | Credenciais |
|---------|-----|-------------|
| **DBT Docs** | http://localhost:8080 (apÃ³s executar `dbt docs serve`) | - |
| **PostgreSQL Source** | localhost:5430 | admin/admin |
| **PostgreSQL Analytics** | localhost:5431 | admin/admin |

## ğŸ“Š **Comandos DBT**

```bash
# Navegar para o projeto DBT
cd dbt_project

# Instalar dependÃªncias
dbt deps

# Executar modelos por camada
dbt run --models tag:bronze    # Camada Bronze
dbt run --models tag:silver    # Camada Silver  
dbt run --models tag:gold      # Camada Gold

# Executar todos os modelos
dbt run

# Executar testes
dbt test

# Gerar documentaÃ§Ã£o
dbt docs generate
dbt docs serve

# ExecuÃ§Ã£o automÃ¡tica com scheduler
python ../scripts/scheduler_dbt.py --interval 300  # A cada 5 minutos
python ../scripts/scheduler_dbt.py --run-once      # Apenas uma vez
```

## ğŸ› ï¸ **Comandos Ãšteis**

```bash
# === GERENCIAMENTO GERAL ===
# Iniciar pipeline completo
./scripts/start_dbt_pipeline.sh

# Apenas construir serviÃ§os
./scripts/start_dbt_pipeline.sh --build-only

# Iniciar com logs
./scripts/start_dbt_pipeline.sh --logs

# Parar todos os serviÃ§os
docker-compose -f config/docker-compose.yml down

# Limpar ambiente (remove volumes)
./scripts/clean_environment.sh

# === LOGS E MONITORAMENTO ===
# Ver logs de todos os serviÃ§os
docker-compose -f config/docker-compose.yml logs -f

# Logs especÃ­ficos
docker-compose -f config/docker-compose.yml logs -f postgres_source
docker-compose -f config/docker-compose.yml logs -f dbt_runner
docker-compose -f config/docker-compose.yml logs -f minio

# === DBT ===
# Executar DBT localmente
cd dbt_project && dbt run

# Executar DBT via container
docker-compose -f config/docker-compose.yml exec dbt_runner dbt run

# Scheduler automÃ¡tico
python scripts/scheduler_dbt.py

# === DADOS ===
# Inserir dados manualmente
python scripts/insere_dados.py

# Conectar ao banco source
psql -h localhost -p 5430 -U admin -d db_source

# === MONITORAMENTO ===
# Status dos containers
docker-compose -f config/docker-compose.yml ps

# Verificar saÃºde dos serviÃ§os
docker-compose -f config/docker-compose.yml exec postgres_source pg_isready -U admin
```

## ğŸ“ **Estrutura do Projeto**

```
â”œâ”€â”€ start_pipeline.sh              # ğŸ¯ Script principal
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ env.config                 # VariÃ¡veis centralizadas
â”‚   â”œâ”€â”€ docker-compose.yml         # ConfiguraÃ§Ã£o completa
â”‚   â”œâ”€â”€ load_env.sh               # Helper para variÃ¡veis
â”‚   â””â”€â”€ README_CREDENCIAIS.md     # DocumentaÃ§Ã£o de credenciais
â”œâ”€â”€ postgres_init_scripts/
â”‚   â””â”€â”€ init_source_db.sql        # Schema do banco source
â”œâ”€â”€ dbt_project/                  # Projeto DBT
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bronze/              # Camada Bronze
â”‚   â”‚   â”œâ”€â”€ silver/              # Camada Silver
â”‚   â”‚   â””â”€â”€ gold/                # Camada Gold
â”‚   â””â”€â”€ dbt_project.yml
â””â”€â”€ dbt_profiles/
    â””â”€â”€ profiles.yml             # ConfiguraÃ§Ã£o DBT
```

## ğŸš¨ **Troubleshooting**

### **Problema: "role admin does not exist"**
```bash
cd config && docker compose down --volumes
docker system prune -f
./start_pipeline.sh
```

### **Problema: DBT nÃ£o encontra tabelas**
1. Verifique se os dados estÃ£o disponÃ­veis no banco de origem:
```bash
docker compose exec postgres_source psql -U admin -d db_source -c "SELECT COUNT(*) FROM clientes;"
```

2. Se retornar erro, execute o script de inserÃ§Ã£o de dados primeiro

### **Problema: Portas ocupadas**
```bash
# Verificar portas em uso
lsof -i :5430 -i :5431 -i :8001 -i :9001

# Matar processos se necessÃ¡rio
killall postgres
docker compose down --remove-orphans
```

## ğŸ”„ **Fluxo de Dados Simplificado**

1. **ğŸ“ APIs inserem dados** â†’ PostgreSQL Source
2. **ğŸ› ï¸ DBT processa diretamente** â†’ Bronze â†’ Silver â†’ Gold
3. **ğŸ“Š Dashboard consome** â†’ Dados transformados
4. **ğŸ”„ Scheduler executa** â†’ Pipeline automatizado

## ğŸ¯ **PrÃ³ximos Passos**

1. Execute `./scripts/start_dbt_pipeline.sh`
2. Aguarde os dados serem inseridos automaticamente
3. Execute transformaÃ§Ãµes DBT
4. Configure o scheduler para execuÃ§Ã£o contÃ­nua
5. Monitore pipeline em tempo real

---

**ğŸ‰ Pipeline DBT pronto para demonstraÃ§Ã£o local com credenciais admin/admin!**